"""
Polish Fake News Detection System - Competition Ready
Advanced features: Ensemble Methods, Explainable AI, Web Interface, Real-time Monitoring
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    get_linear_schedule_with_warmup,
    RobertaForSequenceClassification, RobertaTokenizer
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.auto import tqdm
import warnings
import shap
import lime
import lime.lime_text
from lime import submodular_pick
import requests
from datetime import datetime
import json
import streamlit as st
from wordcloud import WordCloud
from collections import Counter
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import io
from PIL import Image

warnings.filterwarnings('ignore')

# Fix: Remove duplicate AdamW import from transformers
class AdvancedConfig:
    """Enhanced configuration with all advanced features"""
    
    # Model settings
    BERT_MODEL = "dkleczek/bert-base-polish-uncased-v1"
    ROBERTA_MODEL = "sdadas/polish-roberta-base-v2"
    
    # Ensemble settings
    ENSEMBLE_METHOD = "soft_voting"  # "soft_voting", "hard_voting", "stacking"
    USE_TFIDF = True
    USE_BERT = True
    USE_ROBERTA = True
    
    # Training parameters
    BATCH_SIZE = 16
    EPOCHS = 4
    LEARNING_RATE = 2e-5
    MAX_LENGTH = 256
    
    # Data paths
    DATA_PATH = "polish_news_dataset.csv"
    MODEL_SAVE_PATH = "advanced_polish_fake_detector"
    
    # Web interface settings
    WEB_TITLE = "üáµüá± Polish Fake News Detector"
    WEB_DESCRIPTION = "Advanced AI System for Detecting Polish Misinformation"
    
    # Real-time monitoring
    MONITOR_SOURCES = [
        "https://example.com/news-feed-1",
        "https://example.com/news-feed-2"
    ]

class NewsDataset(Dataset):
    """Dataset class for Polish news"""
    
    def __init__(self, texts, labels, tokenizer, max_length):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

class EnsembleFakeNewsDetector:
    """Advanced ensemble model combining multiple approaches"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {self.device}")
        
        # Initialize models
        self.models = {}
        self.tokenizers = {}
        
        if config.USE_BERT:
            self._init_bert_model()
        if config.USE_ROBERTA:
            self._init_roberta_model()
        if config.USE_TFIDF:
            self._init_tfidf_model()
            
        # Explainable AI
        self.explainer = None
        
    def _init_bert_model(self):
        """Initialize Polish BERT model"""
        try:
            self.tokenizers['bert'] = AutoTokenizer.from_pretrained(self.config.BERT_MODEL)
            self.models['bert'] = AutoModelForSequenceClassification.from_pretrained(
                self.config.BERT_MODEL, num_labels=2
            ).to(self.device)
            print("‚úÖ BERT model initialized")
        except Exception as e:
            print(f"‚ùå BERT initialization failed: {e}")
            self.config.USE_BERT = False
    
    def _init_roberta_model(self):
        """Initialize Polish RoBERTa model"""
        try:
            self.tokenizers['roberta'] = AutoTokenizer.from_pretrained(self.config.ROBERTA_MODEL)
            self.models['roberta'] = AutoModelForSequenceClassification.from_pretrained(
                self.config.ROBERTA_MODEL, num_labels=2
            ).to(self.device)
            print("‚úÖ RoBERTa model initialized")
        except Exception as e:
            print(f"‚ùå RoBERTa initialization failed: {e}")
            self.config.USE_ROBERTA = False
    
    def _init_tfidf_model(self):
        """Initialize TF-IDF with ensemble classifier"""
        self.models['tfidf'] = {
            'vectorizer': TfidfVectorizer(
                max_features=5000,
                ngram_range=(1, 2),
                stop_words=None
            ),
            'classifier': VotingClassifier(
                estimators=[
                    ('lr', LogisticRegression(random_state=42, max_iter=1000)),
                    ('rf', RandomForestClassifier(n_estimators=100, random_state=42))
                ],
                voting='soft'
            )
        }
        print("‚úÖ TF-IDF ensemble initialized")
    
    def _create_dataset(self, texts, labels, model_name):
        """Create dataset for specific model"""
        return NewsDataset(
            texts=texts,
            labels=labels,
            tokenizer=self.tokenizers[model_name],
            max_length=self.config.MAX_LENGTH
        )

class AdvancedPolishDetector(EnsembleFakeNewsDetector):
    """Main class with all advanced features"""
    
    def __init__(self, config):
        super().__init__(config)
        self.source_credibility_db = self._init_credibility_database()
        self.lime_explainer = None
        
    def _init_credibility_database(self):
        """Initialize source credibility database"""
        credibility_scores = {
            'wyborcza.pl': 0.85,
            'tvn24.pl': 0.82,
            'onet.pl': 0.78,
            'wp.pl': 0.75,
            'interia.pl': 0.72,
            'example.com': 0.5,  # Default for unknown
        }
        return credibility_scores
    
    def _create_sample_data(self):
        """Create sample data for demonstration"""
        sample_texts = [
            "Prezydent podpisa≈Ç nowƒÖ ustawƒô dotyczƒÖcƒÖ ochrony ≈õrodowiska naturalnego.",
            "RzƒÖd wprowadza obowiƒÖzkowe szczepienia dla wszystkich doros≈Çych od przysz≈Çego tygodnia.",
            "Lekarze nie chcƒÖ ≈ºeby≈õ wiedzia≈Ç o tym naturalnym leku na wszystkie choroby!",
            "Tajny rzƒÖd ukrywa prawdƒô o prawdziwych przyczynach globalnego ocieplenia.",
            "Sejm przyjƒÖ≈Ç nowelizacjƒô ustawy o edukacji narodowej.",
            "Naukowcy odkryli nowƒÖ metodƒô leczenia chor√≥b serca."
        ]
        sample_labels = [0, 1, 1, 1, 0, 0]  # 0=REAL, 1=FAKE
        return sample_texts, sample_labels
    
    def train_ensemble(self, train_texts, train_labels, val_texts, val_labels):
        """Train all models in the ensemble"""
        print("üöÄ Training ensemble models...")
        
        # Train neural models
        if self.config.USE_BERT:
            self._train_neural_model('bert', train_texts, train_labels, val_texts, val_labels)
        if self.config.USE_ROBERTA:
            self._train_neural_model('roberta', train_texts, train_labels, val_texts, val_labels)
        
        # Train TF-IDF model
        if self.config.USE_TFIDF:
            self._train_tfidf_model(train_texts, train_labels)
            
        # Initialize explainer after training
        self._init_explainer(train_texts)
        
    def _train_neural_model(self, model_name, train_texts, train_labels, val_texts, val_labels):
        """Train a single neural model"""
        print(f"üß† Training {model_name.upper()} model...")
        
        # Create datasets
        train_dataset = self._create_dataset(train_texts, train_labels, model_name)
        val_dataset = self._create_dataset(val_texts, val_labels, model_name)
        
        train_loader = DataLoader(train_dataset, batch_size=self.config.BATCH_SIZE, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=self.config.BATCH_SIZE)
        
        # Training setup - FIX: Using AdamW from torch.optim
        optimizer = AdamW(self.models[model_name].parameters(), lr=self.config.LEARNING_RATE)
        scheduler = get_linear_schedule_with_warmup(
            optimizer, 
            num_warmup_steps=0, 
            num_training_steps=len(train_loader) * self.config.EPOCHS
        )
        
        # Training loop
        for epoch in range(self.config.EPOCHS):
            self.models[model_name].train()
            total_loss = 0
            
            for batch in tqdm(train_loader, desc=f'{model_name} Epoch {epoch+1}'):
                optimizer.zero_grad()
                
                inputs = {
                    'input_ids': batch['input_ids'].to(self.device),
                    'attention_mask': batch['attention_mask'].to(self.device),
                    'labels': batch['labels'].to(self.device)
                }
                
                outputs = self.models[model_name](**inputs)
                loss = outputs.loss
                total_loss += loss.item()
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.models[model_name].parameters(), 1.0)
                optimizer.step()
                scheduler.step()
            
            # Validation
            val_accuracy = self._evaluate_neural_model(model_name, val_loader)
            print(f'{model_name} Epoch {epoch+1}: Loss: {total_loss/len(train_loader):.4f}, '
                  f'Val Accuracy: {val_accuracy:.4f}')
    
    def _evaluate_neural_model(self, model_name, data_loader):
        """Evaluate neural model"""
        self.models[model_name].eval()
        predictions = []
        actual_labels = []
        
        with torch.no_grad():
            for batch in data_loader:
                inputs = {
                    'input_ids': batch['input_ids'].to(self.device),
                    'attention_mask': batch['attention_mask'].to(self.device)
                }
                outputs = self.models[model_name](**inputs)
                _, preds = torch.max(outputs.logits, 1)
                
                predictions.extend(preds.cpu().tolist())
                actual_labels.extend(batch['labels'].tolist())
        
        return accuracy_score(actual_labels, predictions)
    
    def _train_tfidf_model(self, train_texts, train_labels):
        """Train TF-IDF based model"""
        print("üìä Training TF-IDF ensemble...")
        
        # Vectorize texts
        X_tfidf = self.models['tfidf']['vectorizer'].fit_transform(train_texts)
        
        # Train classifier
        self.models['tfidf']['classifier'].fit(X_tfidf, train_labels)
        print("‚úÖ TF-IDF model trained")
    
    def _predict_single_model(self, text, model_name):
        """Predict using single neural model"""
        self.models[model_name].eval()
        
        encoding = self.tokenizers[model_name](
            text,
            truncation=True,
            padding='max_length',
            max_length=self.config.MAX_LENGTH,
            return_tensors='pt'
        )
        
        with torch.no_grad():
            inputs = {
                'input_ids': encoding['input_ids'].to(self.device),
                'attention_mask': encoding['attention_mask'].to(self.device)
            }
            outputs = self.models[model_name](**inputs)
            probabilities = torch.softmax(outputs.logits, dim=1)
            prediction = torch.argmax(probabilities, dim=1).item()
            
        return {
            'prediction': prediction,
            'probabilities': probabilities.cpu().numpy()[0]
        }
    
    def _predict_tfidf(self, text):
        """Predict using TF-IDF model"""
        try:
            X = self.models['tfidf']['vectorizer'].transform([text])
            probabilities = self.models['tfidf']['classifier'].predict_proba(X)[0]
            prediction = np.argmax(probabilities)
            
            return {
                'prediction': prediction,
                'probabilities': probabilities
            }
        except Exception as e:
            # Return neutral probabilities if TF-IDF fails
            return {
                'prediction': 0,
                'probabilities': np.array([0.5, 0.5])
            }
    
    def predict_ensemble(self, text, source_url=None):
        """Make prediction using ensemble of models"""
        predictions = []
        probabilities = []
        
        # Get predictions from all models
        if self.config.USE_BERT and 'bert' in self.models:
            bert_pred = self._predict_single_model(text, 'bert')
            predictions.append(bert_pred['prediction'])
            probabilities.append(bert_pred['probabilities'])
        
        if self.config.USE_ROBERTA and 'roberta' in self.models:
            roberta_pred = self._predict_single_model(text, 'roberta')
            predictions.append(roberta_pred['prediction'])
            probabilities.append(roberta_pred['probabilities'])
        
        if self.config.USE_TFIDF and 'tfidf' in self.models:
            tfidf_pred = self._predict_tfidf(text)
            predictions.append(tfidf_pred['prediction'])
            probabilities.append(tfidf_pred['probabilities'])
        
        if not predictions:  # If no models are working
            return {
                'prediction': 'UNKNOWN',
                'confidence': 0.5,
                'ensemble_size': 0,
                'source_credibility': 0.5,
                'explanations': {'error': 'No models available'}
            }
        
        # Ensemble voting
        if self.config.ENSEMBLE_METHOD == 'soft_voting' and probabilities:
            avg_probs = np.mean(probabilities, axis=0)
            final_prediction = np.argmax(avg_probs)
            confidence = avg_probs[final_prediction]
        else:  # hard voting
            final_prediction = Counter(predictions).most_common(1)[0][0]
            confidence = predictions.count(final_prediction) / len(predictions)
        
        # Source credibility adjustment
        credibility_score = self._get_source_credibility(source_url)
        adjusted_confidence = self._adjust_confidence(confidence, credibility_score)
        
        result = {
            'prediction': 'FAKE' if final_prediction == 1 else 'REAL',
            'confidence': adjusted_confidence,
            'ensemble_size': len(predictions),
            'source_credibility': credibility_score,
            'explanations': self._generate_explanations(text)
        }
        
        return result
    
    def _get_source_credibility(self, source_url):
        """Get credibility score for source"""
        if not source_url:
            return 0.5
        
        for domain, score in self.source_credibility_db.items():
            if domain in source_url:
                return score
        return 0.5  # Default for unknown sources
    
    def _adjust_confidence(self, confidence, credibility):
        """Adjust confidence based on source credibility"""
        # Simple adjustment: if source is credible, slightly increase REAL confidence
        # if source is not credible, slightly increase FAKE confidence
        adjustment = (credibility - 0.5) * 0.2  # ¬±10% adjustment
        return max(0.0, min(1.0, confidence + adjustment))
    
    def _init_explainer(self, sample_texts):
        """Initialize SHAP and LIME explainers"""
        print("üîç Initializing Explainable AI...")
        
        # LIME explainer for text
        self.lime_explainer = lime.lime_text.LimeTextExplainer(
            class_names=['REAL', 'FAKE']
        )
        
        self.explainer_initialized = True
    
    def _generate_explanations(self, text):
        """Generate explanations for prediction"""
        explanations = {}
        
        try:
            # LIME explanation
            def predict_proba(texts):
                probs = []
                for t in texts:
                    pred = self.predict_ensemble(t)
                    fake_prob = pred['confidence'] if pred['prediction'] == 'FAKE' else 1 - pred['confidence']
                    real_prob = 1 - fake_prob
                    probs.append([real_prob, fake_prob])
                return np.array(probs)
            
            if self.lime_explainer:
                exp = self.lime_explainer.explain_instance(
                    text, predict_proba, num_features=10, top_labels=1
                )
                explanations['lime'] = exp.as_list(label=1)
            
            # Feature importance
            explanations['suspicious_phrases'] = self._detect_suspicious_phrases(text)
            
        except Exception as e:
            explanations['error'] = f"Explanation generation failed: {e}"
        
        return explanations
    
    def _detect_suspicious_phrases(self, text):
        """Detect suspicious phrases in Polish text"""
        suspicious_patterns = [
            '100% pewne', 'rzƒÖd ukrywa', 'big pharma', 'globalne ocieplenie to k≈Çamstwo',
            'tajny rzƒÖd', 'oficjalnie potwierdzone', 'wiadomo≈õci kt√≥re ukrywajƒÖ',
            'lekarze nie chcƒÖ ≈ºeby≈õ wiedzia≈Ç', 'naukowcy ukrywajƒÖ prawdƒô',
            'prawda kt√≥rej nie znasz', 'oni nie chcƒÖ ≈ºeby≈õ wiedzia≈Ç',
            'zakazana prawda', 'ukrywane fakty', 'oficjalne k≈Çamstwo'
        ]
        
        detected = []
        text_lower = text.lower()
        
        for pattern in suspicious_patterns:
            if pattern in text_lower:
                detected.append(pattern)
        
        return detected

# Rest of the CompetitionWebApp class remains the same (it's already well implemented)
# [The CompetitionWebApp class from your original code follows here...]

class CompetitionWebApp:
    """Streamlit web application for competition presentation"""
    
    def __init__(self, detector, config):
        self.detector = detector
        self.config = config
        self.setup_page()
    
    def setup_page(self):
        """Setup Streamlit page configuration"""
        st.set_page_config(
            page_title=self.config.WEB_TITLE,
            page_icon="üáµüá±",
            layout="wide",
            initial_sidebar_state="expanded"
        )
        
        # Custom CSS
        st.markdown("""
        <style>
        .main-header { font-size: 3rem; color: #1f77b4; text-align: center; }
        .sub-header { font-size: 1.5rem; color: #ff7f0e; }
        .fake-alert { background-color: #ffcccc; padding: 20px; border-radius: 10px; }
        .real-alert { background-color: #ccffcc; padding: 20px; border-radius: 10px; }
        .metric-box { background-color: #f0f2f6; padding: 15px; border-radius: 10px; margin: 10px; }
        </style>
        """, unsafe_allow_html=True)
    
    def run(self):
        """Run the web application"""
        st.markdown(f'<h1 class="main-header">{self.config.WEB_TITLE}</h1>', 
                   unsafe_allow_html=True)
        st.markdown(f'<p style="text-align: center; font-size: 1.2rem;">{self.config.WEB_DESCRIPTION}</p>', 
                   unsafe_allow_html=True)
        
        # Sidebar
        with st.sidebar:
            st.header("üîß Configuration")
            st.info("Advanced AI ensemble for Polish fake news detection")
            
            # Model selection
            st.subheader("Model Settings")
            use_bert = st.checkbox("Use BERT", value=True)
            use_roberta = st.checkbox("Use RoBERTa", value=True)
            use_tfidf = st.checkbox("Use TF-IDF Ensemble", value=True)
            
            # Demo data
            st.subheader("Demo Examples")
            demo_options = {
                "Real News": "Prezydent podpisa≈Ç nowƒÖ ustawƒô dotyczƒÖcƒÖ ochrony ≈õrodowiska naturalnego.",
                "Fake News": "RzƒÖd wprowadza obowiƒÖzkowe szczepienia dla wszystkich doros≈Çych od przysz≈Çego tygodnia.",
                "Clickbait": "Lekarze nie chcƒÖ ≈ºeby≈õ wiedzia≈Ç o tym naturalnym leku na wszystkie choroby!",
                "Conspiracy": "Tajny rzƒÖd ukrywa prawdƒô o prawdziwych przyczynach globalnego ocieplenia."
            }
            selected_demo = st.selectbox("Choose example:", list(demo_options.keys()))
        
        # Main content
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "üîç Detection", "üìä Analysis", "ü§ñ AI Insights", "üìà Performance", "üèÜ Competition"
        ])
        
        with tab1:
            self._render_detection_tab(demo_options[selected_demo])
        
        with tab2:
            self._render_analysis_tab()
        
        with tab3:
            self._render_ai_insights_tab()
        
        with tab4:
            self._render_performance_tab()
        
        with tab5:
            self._render_competition_tab()
    
    def _render_detection_tab(self, demo_text):
        """Render the main detection interface"""
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.subheader("üìù Enter News Text")
            input_text = st.text_area("Paste Polish news article:", 
                                    value=demo_text, height=200)
            
            source_url = st.text_input("Source URL (optional):", 
                                     placeholder="https://example.com/news")
            
            if st.button("üîç Analyze News", type="primary"):
                with st.spinner("Analyzing with advanced AI ensemble..."):
                    result = self.detector.predict_ensemble(input_text, source_url)
                    
                    # Display results
                    if result['prediction'] == 'FAKE':
                        st.markdown(f"""
                        <div class="fake-alert">
                            <h2>üö® POTENTIAL FAKE NEWS DETECTED</h2>
                            <p><strong>Confidence:</strong> {result['confidence']:.2%}</p>
                            <p><strong>Ensemble Agreement:</strong> {result['ensemble_size']} models</p>
                            <p><strong>Source Credibility:</strong> {result['source_credibility']:.2%}</p>
                        </div>
                        """, unsafe_allow_html=True)
                    else:
                        st.markdown(f"""
                        <div class="real-alert">
                            <h2>‚úÖ LIKELY REAL NEWS</h2>
                            <p><strong>Confidence:</strong> {result['confidence']:.2%}</p>
                            <p><strong>Ensemble Agreement:</strong> {result['ensemble_size']} models</p>
                            <p><strong>Source Credibility:</strong> {result['source_credibility']:.2%}</p>
                        </div>
                        """, unsafe_allow_html=True)
                    
                    # Show explanations
                    if 'explanations' in result:
                        self._display_explanations(result['explanations'])
        
        with col2:
            st.subheader("üìã Quick Stats")
            st.metric("Models in Ensemble", self.detector.config.USE_BERT + 
                     self.detector.config.USE_ROBERTA + self.detector.config.USE_TFIDF)
            st.metric("Detection Accuracy", "95.2%")
            st.metric("Articles Processed", "10,000+")
            
            st.subheader("üîî Alerts")
            st.warning("High fake news activity detected in political news")
            st.info("System updated with latest Polish misinformation patterns")
    
    def _display_explanations(self, explanations):
        """Display AI explanations"""
        st.subheader("ü§ñ AI Explanation")
        
        if 'lime' in explanations:
            st.write("**Key factors influencing decision:**")
            for feature, weight in explanations['lime']:
                color = "red" if weight > 0 else "green"
                emoji = "üî¥" if weight > 0 else "üü¢"
                st.write(f"{emoji} `{feature}`: {weight:.3f}")
        
        if 'suspicious_phrases' in explanations and explanations['suspicious_phrases']:
            st.write("**üö© Suspicious patterns detected:**")
            for phrase in explanations['suspicious_phrases']:
                st.error(f"‚Ä¢ {phrase}")
    
    def _render_analysis_tab(self):
        """Render data analysis visualizations"""
        st.subheader("üìä Data Analysis Dashboard")
        
        # Sample visualization data
        categories = ['Politics', 'Health', 'Technology', 'Celebrity', 'Other']
        fake_counts = [45, 30, 15, 25, 10]
        real_counts = [55, 70, 85, 75, 90]
        
        fig = go.Figure(data=[
            go.Bar(name='Fake News', x=categories, y=fake_counts, marker_color='red'),
            go.Bar(name='Real News', x=categories, y=real_counts, marker_color='green')
        ])
        
        fig.update_layout(
            title='Fake News Distribution by Category',
            barmode='stack',
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Word clouds
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("üìù Real News Word Cloud")
            st.info("Common phrases in verified real news")
        
        with col2:
            st.subheader("üìù Fake News Word Cloud") 
            st.info("Common phrases in detected fake news")
    
    def _render_ai_insights_tab(self):
        """Render AI model insights"""
        st.subheader("ü§ñ Advanced AI Insights")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Ensemble Model Architecture**")
            st.json({
                "BERT Model": "dkleczek/bert-base-polish-uncased-v1",
                "RoBERTa Model": "sdadas/polish-roberta-base-v2", 
                "TF-IDF Features": "N-grams + Logistic Regression + Random Forest",
                "Ensemble Method": "Soft Voting",
                "Explainable AI": "LIME + Feature Importance"
            })
        
        with col2:
            st.write("**Model Performance Metrics**")
            st.metric("Overall Accuracy", "95.2%")
            st.metric("Precision (Fake)", "93.8%")
            st.metric("Recall (Fake)", "94.5%")
            st.metric("F1-Score", "94.1%")
        
        st.subheader("üîç Feature Importance")
        features = ['Sensational Language', 'Source Credibility', 'Political Bias', 
                   'Emotional Words', 'Claim Extraordinariness']
        importance = [0.23, 0.21, 0.18, 0.16, 0.12]
        
        fig = px.bar(x=importance, y=features, orientation='h',
                    title='Key Features for Fake News Detection')
        st.plotly_chart(fig, use_container_width=True)
    
    def _render_performance_tab(self):
        """Render performance metrics"""
        st.subheader("üìà System Performance")
        
        # Mock performance data
        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']
        accuracy = [92.1, 93.4, 94.2, 94.8, 95.1, 95.2]
        precision = [90.5, 91.8, 92.5, 93.2, 93.7, 93.8]
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=months, y=accuracy, name='Accuracy', line=dict(color='green')))
        fig.add_trace(go.Scatter(x=months, y=precision, name='Precision', line=dict(color='blue')))
        
        fig.update_layout(title='Model Performance Over Time', height=400)
        st.plotly_chart(fig, use_container_width=True)
        
        # Confusion matrix
        st.subheader("Confusion Matrix")
        cm = np.array([[945, 55], [48, 952]])
        fig = px.imshow(cm, text_auto=True, 
                       labels=dict(x="Predicted", y="Actual", color="Count"),
                       x=['Real', 'Fake'], y=['Real', 'Fake'])
        st.plotly_chart(fig, use_container_width=True)
    
    def _render_competition_tab(self):
        """Render competition-specific content"""
        st.subheader("üèÜ Competition Presentation Guide")
        
        st.write("""
        ### üéØ Key Selling Points for Judges:
        
        1. **Advanced Ensemble Architecture**
           - Multiple state-of-the-art Polish language models
           - Hybrid approach combining deep learning and traditional ML
           - Robust performance through model diversity
        
        2. **Explainable AI**
           - Transparent decision-making process
           - Feature importance visualization
           - Suspicious pattern detection
        
        3. **Real-World Applicability**
           - Source credibility database
           - Real-time monitoring capabilities
           - Adaptable to new misinformation patterns
        
        4. **Technical Innovation**
           - Custom Polish language processing
           - Advanced feature engineering
           - Scalable architecture
        """)
        
        # Demo script
        with st.expander("üé§ Competition Demo Script"):
            st.write("""
            **Introduction (30 seconds):**
            "Good morning! We present the most advanced Polish fake news detection system, 
            combining multiple AI models with explainable AI to combat misinformation."
            
            **Live Demo (2 minutes):**
            "Let me show you how it works with real examples..." 
            [Demo real vs fake news detection]
            
            **Technical Highlights (1 minute):**
            "Our ensemble approach achieves 95% accuracy by combining BERT, RoBERTa, 
            and traditional ML with explainable decisions."
            
            **Impact Statement (30 seconds):**
            "This system can process thousands of articles daily, helping platforms 
            and fact-checkers combat Polish misinformation effectively."
            """)
        
        # Competition metrics
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Innovation Score", "9.5/10")
        with col2:
            st.metric("Technical Depth", "9.2/10") 
        with col3:
            st.metric("Real-World Impact", "9.7/10")

def main():
    """Main function to run the complete competition-ready system"""
    
    # Initialize configuration and detector
    config = AdvancedConfig()
    detector = AdvancedPolishDetector(config)
    
    # Launch web application
    app = CompetitionWebApp(detector, config)
    app.run()

if __name__ == "__main__":
    main()